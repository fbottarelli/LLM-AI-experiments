{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New working directory: /home/fd/repo/AI/LLM-AI-experiments\n"
     ]
    }
   ],
   "source": [
    "# modifica al path del progetto\n",
    "import os\n",
    "# Modifica il percorso di lavoro se necessario\n",
    "os.chdir('..')\n",
    "# Stampa di nuovo il percorso di lavoro per conferma\n",
    "print(\"New working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load env\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "# TODO(developer): Update and un-comment below line\n",
    "project_id = \"geminitesting-432015\"\n",
    "location = \"europe-west1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some names for a flower shop specializing in dried flowers, playing on different themes:\n",
      "\n",
      "**Nature & Timelessness:**\n",
      "\n",
      "* **Everlasting Bloom**\n",
      "* **Dried & Divine**\n",
      "* **Wildflower Preserve**\n",
      "* **The Timeless Bouquet**\n",
      "* **Bloom Forever**\n",
      "* **Sun-Kissed Petals**\n",
      "* **Whispers of Autumn**\n",
      "* **The Dried Flower Garden**\n",
      "* **Eternal Blooms**\n",
      "* **Botanical Memories**\n",
      "\n",
      "**Vintage & Romantic:**\n",
      "\n",
      "* **The Dried Flower Co.**\n",
      "* **The Dusty Bloom**\n",
      "* **Vintage Vases**\n",
      "* **The Heirloom Bouquet**\n",
      "* **The Dried Petal Atelier**\n",
      "* **A Touch of Nostalgia**\n",
      "* **Bloom & Lace**\n",
      "* **The Rustic Bouquet**\n",
      "* **Forever in Bloom**\n",
      "* **The Petal Whisperer**\n",
      "\n",
      "**Modern & Chic:**\n",
      "\n",
      "* **Dried & Designed**\n",
      "* **The Modern Bloom**\n",
      "* **The Botanical Studio**\n",
      "* **The Petal Collective**\n",
      "* **Minimal Blooms**\n",
      "* **The Dried Flower Bar**\n",
      "* **Ethereal Blooms**\n",
      "* **The Paper Petal**\n",
      "* **Bloom & Soul**\n",
      "* **The Dried Flower Gallery**\n",
      "\n",
      "**Unique & Playful:**\n",
      "\n",
      "* **The Flower Whisperer**\n",
      "* **The Dried Petal Project**\n",
      "* **The Bloom Alchemist**\n",
      "* **The Flower Fairy**\n",
      "* **The Dried Flower Muse**\n",
      "* **The Bouquet Botanist**\n",
      "* **The Petal Alchemist**\n",
      "* **The Dried Flower Apothecary**\n",
      "* **The Floral Muse**\n",
      "\n",
      "**Tips for Choosing the Right Name:**\n",
      "\n",
      "* **Keep it short and memorable.**\n",
      "* **Make sure it's easy to pronounce and spell.**\n",
      "* **Reflect your brand's personality and style.**\n",
      "* **Check for availability of the name and domain name.**\n",
      "\n",
      "Ultimately, the best name will be one that resonates with you and your target audience. Good luck! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "vertexai.init(project=project_id, location=\"us-central1\")\n",
    "\n",
    "model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
    "\n",
    "response = model.generate_content(\n",
    "    \"What's a good name for a flower shop that specializes in selling bouquets of dried flowers?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a plate of blueberry scones with a cup of coffee and a bowl of blueberries. It looks like a delicious and inviting breakfast or brunch spread.\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "\n",
    "# TODO(developer): Update and un-comment below line\n",
    "# project_id = \"PROJECT_ID\"\n",
    "\n",
    "vertexai.init(project=project_id, location=location)\n",
    "\n",
    "model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
    "\n",
    "response = model.generate_content(\n",
    "    [\n",
    "        Part.from_uri(\n",
    "            \"gs://cloud-samples-data/generative-ai/image/scones.jpg\",\n",
    "            mime_type=\"image/jpeg\",\n",
    "        ),\n",
    "        \"What is shown in this image?\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "\n",
    "# TODO(developer): Update and un-comment below lines\n",
    "# project_id = \"PROJECT_ID\"\n",
    "\n",
    "vertexai.init(project=project_id, location=location)\n",
    "\n",
    "model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
    "\n",
    "prompt = \n",
    "audio_file_path = \"media/gemini/media/filename.mp3\"\n",
    "\n",
    "# Read the audio file in binary mode\n",
    "with open(audio_file_path, \"rb\") as audio_file:\n",
    "    audio_bytes = audio_file.read()\n",
    "\n",
    "audio_part = Part.from_data(audio_bytes, mime_type=\"audio/mp3\")\n",
    "\n",
    "contents = [audio_part, prompt]\n",
    "\n",
    "# Counts tokens\n",
    "print(model.count_tokens(contents))\n",
    "response = model.generate_content(contents)\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
