{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New working directory: /home/fd/repo/not-work-repo/LLM-AI-experiments\n"
     ]
    }
   ],
   "source": [
    "# modifica al path del progetto\n",
    "import os\n",
    "# Modifica il percorso di lavoro se necessario\n",
    "os.chdir('..')\n",
    "# Stampa di nuovo il percorso di lavoro per conferma\n",
    "print(\"New working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "project_id = os.getenv(\"gemini_project_id\")\n",
    "location = os.getenv(\"gemini_location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How use Class Part\n",
    "```\n",
    "text_part = Part.from_text(\"Why is sky blue?\")\n",
    "image_part = Part.from_image(Image.load_from_file(\"image.jpg\"))\n",
    "video_part = Part.from_uri(uri=\"gs://.../video.mp4\", mime_type=\"video/mp4\")\n",
    "function_response_part = Part.from_function_response(\n",
    "    name=\"get_current_weather\",\n",
    "    response={\n",
    "        \"content\": {\"weather_there\": \"super nice\"},\n",
    "    }\n",
    ")\n",
    "\n",
    "response1 = model.generate_content([text_part, image_part])\n",
    "response2 = model.generate_content(video_part)\n",
    "response3 = chat.send_message(function_response_part)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available methods to create a Part instance:\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_from_gapic', '_image', 'file_data', 'from_data', 'from_dict', 'from_function_response', 'from_image', 'from_text', 'from_uri', 'function_call', 'function_response', 'inline_data', 'mime_type', 'text', 'to_dict']\n",
      "\n",
      "\n",
      "Available methods to create a Part instance and their parameters:\n",
      "_from_gapic(raw_part: raw_part: google.cloud.aiplatform_v1beta1.types.content.Part)\n",
      "from_data(data: data: bytes, mime_type: mime_type: str)\n",
      "from_dict(part_dict: part_dict: Dict[str, Any])\n",
      "from_function_response(name: name: str, response: response: Dict[str, Any])\n",
      "from_image(image: image: 'Image')\n",
      "from_text(text: text: str)\n",
      "from_uri(uri: uri: str, mime_type: mime_type: str)\n",
      "to_dict(self: self)\n"
     ]
    }
   ],
   "source": [
    "# To explore all the possible methods to create a Part instance, we can list the available class methods of the Part class.\n",
    "# This will include methods like from_text, from_image, from_uri, and from_function_response.\n",
    "# Additionally, we can inspect the parameters required by each method using the inspect module.\n",
    "\n",
    "# We can use the dir() function on the Part class itself to see all the class methods.\n",
    "print(\"Available methods to create a Part instance:\")\n",
    "print([method for method in dir(Part)])\n",
    "\n",
    "import inspect\n",
    "\n",
    "print(\"\\n\")\n",
    "# Display available methods to create a Part instance along with their parameters in a more readable format\n",
    "print(\"Available methods to create a Part instance and their parameters:\")\n",
    "for method in dir(Part):\n",
    "    if callable(getattr(Part, method)) and not method.startswith(\"__\"):\n",
    "        params = inspect.signature(getattr(Part, method)).parameters\n",
    "        param_list = ', '.join(f\"{name}: {param}\" for name, param in params.items())\n",
    "        print(f\"{method}({param_list})\")\n",
    "\n",
    "# Additionally, check if 'file_data' method exists and print its parameters if it does\n",
    "if hasattr(Part, 'file_data') and callable(getattr(Part, 'file_data')):\n",
    "    file_data_params = inspect.signature(getattr(Part, 'file_data')).parameters\n",
    "    file_data_param_list = ', '.join(f\"{name}: {param}\" for name, param in file_data_params.items())\n",
    "    print(f\"file_data({file_data_param_list})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Advanced RAG Techniques: Obsidian Links for Deeper Exploration \n",
      "\n",
      "Here's your concise list with Obsidian links for easy access: \n",
      "\n",
      "**Core Techniques:**\n",
      "\n",
      "- **[[RAG - Simple RAG|Simple RAG]]**: Basic retrieval & integration. \n",
      "- **[[RAG - Context Enrichment Techniques|Context Enrichment]]**: Enhancing retrieval with neighboring sentences.\n",
      "- **[[RAG - Multi-faceted Filtering|Multi-faceted Filtering]]**: Refining results with metadata, similarity, content, and diversity filters.\n",
      "- **[[RAG - Fusion Retrieval|Fusion Retrieval]]**: Combining retrieval methods for comprehensive results. \n",
      "- **[[RAG - Intelligent Reranking|Intelligent Reranking]]**: Improving relevance ranking with advanced scoring.\n",
      "\n",
      "**Advanced Retrieval & Optimization:**\n",
      "\n",
      "- **[[RAG - Query Transformations|Query Transformations]]**: Modifying queries for better retrieval.\n",
      "- **[[RAG - Hierarchical Indices|Hierarchical Indices]]**: Multi-tiered systems for efficient navigation. \n",
      "- **[[RAG - Hypothetical Questions (HyDE Approach)|Hypothetical Questions (HyDE)]]**: Generating questions to improve query-data alignment.\n",
      "- **[[RAG - Choose Chunk Size|Choose Chunk Size]]**: Balancing context and retrieval speed.\n",
      "- **[[RAG - Semantic Chunking|Semantic Chunking]]**: Dividing text based on meaning. \n",
      "- **[[RAG - Contextual Compression|Contextual Compression]]**: Summarizing while preserving relevance. \n",
      "- **[[RAG - Explainable Retrieval|Explainable Retrieval]]**: Providing transparency in the process.\n",
      "- **[[RAG - Retrieval with Feedback Loops|Retrieval with Feedback Loops]]**:  Learning from user interactions. \n",
      "- **[[RAG - Adaptive Retrieval|Adaptive Retrieval]]**: Adjusting strategies based on query and context.\n",
      "\n",
      "**Cutting-Edge Approaches:**\n",
      "\n",
      "- **[[RAG - Iterative Retrieval|Iterative Retrieval]]**: Multiple retrieval rounds for refinement. \n",
      "- **[[RAG - Ensemble Retrieval|Ensemble Retrieval]]**: Combining models for robustness. \n",
      "- **[[RAG - Knowledge Graph Integration (Graph RAG)|Knowledge Graph Integration (Graph RAG)]]**: Enriching context with structured data.\n",
      "- **[[RAG - Multi-modal Retrieval|Multi-modal Retrieval]]**: Handling diverse data types.\n",
      "- **[[RAG - RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval|RAPTOR]]**:  Recursive processing for tree-structured information. \n",
      "- **[[RAG - Self RAG|Self RAG]]**: Dynamically choosing between retrieval and generation. \n",
      "- **[[RAG - Corrective RAG|Corrective RAG]]**: Evaluating and correcting retrieval on the fly.\n",
      "- **[[RAG - Document Augmentation through Question Generation for Enhanced Retrieval|Document Augmentation]]**: Enhancing retrieval with generated questions.\n",
      "- **[[RAG - Sophisticated Controllable Agent for Complex RAG Tasks|Sophisticated Controllable Agent]]**: Handling complex tasks with a deterministic graph-based agent. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "vertexai.init(project=project_id, location=\"europe-west1\")\n",
    "\n",
    "model = GenerativeModel(\"gemini-1.5-pro-001\")\n",
    "\n",
    "input_text = '''\n",
    "Advanced Techniques\n",
    "Explore the extensive list of cutting-edge RAG techniques:\n",
    "\n",
    "1. Simple RAG üå±\n",
    "LangChain\n",
    "LlamaIndex\n",
    "Overview üîé\n",
    "Introducing basic RAG techniques ideal for newcomers.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "Start with basic retrieval queries and integrate incremental learning mechanisms.\n",
    "\n",
    "2. Context Enrichment Techniques üìù\n",
    "Overview üîé\n",
    "Enhancing retrieval accuracy by embedding individual sentences and extending context to neighboring sentences.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "Retrieve the most relevant sentence while also accessing the sentences before and after it in the original text.\n",
    "\n",
    "3. Multi-faceted Filtering üîç\n",
    "Overview üîé\n",
    "Applying various filtering techniques to refine and improve the quality of retrieved results.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "üè∑Ô∏è Metadata Filtering: Apply filters based on attributes like date, source, author, or document type.\n",
    "üìä Similarity Thresholds: Set thresholds for relevance scores to keep only the most pertinent results.\n",
    "üìÑ Content Filtering: Remove results that don't match specific content criteria or essential keywords.\n",
    "üåà Diversity Filtering: Ensure result diversity by filtering out near-duplicate entries.\n",
    "4. Fusion Retrieval üîó\n",
    "Overview üîé\n",
    "Optimizing search results by combining different retrieval methods.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "Combine keyword-based search with vector-based search for more comprehensive and accurate retrieval.\n",
    "\n",
    "5. Intelligent Reranking üìà\n",
    "Overview üîé\n",
    "Applying advanced scoring mechanisms to improve the relevance ranking of retrieved results.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "üß† LLM-based Scoring: Use a language model to score the relevance of each retrieved chunk.\n",
    "üîÄ Cross-Encoder Models: Re-encode both the query and retrieved documents jointly for similarity scoring.\n",
    "üèÜ Metadata-enhanced Ranking: Incorporate metadata into the scoring process for more nuanced ranking.\n",
    "6.Query Transformations üîÑ\n",
    "Overview üîé\n",
    "Modifying and expanding queries to improve retrieval effectiveness.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "‚úçÔ∏è Query Rewriting: Reformulate queries to improve retrieval.\n",
    "üîô Step-back Prompting: Generate broader queries for better context retrieval.\n",
    "üß© Sub-query Decomposition: Break complex queries into simpler sub-queries.\n",
    "7. Hierarchical Indices üóÇÔ∏è\n",
    "Overview üîé\n",
    "Creating a multi-tiered system for efficient information navigation and retrieval.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "Implement a two-tiered system for document summaries and detailed chunks, both containing metadata pointing to the same location in the data.\n",
    "\n",
    "8. Hypothetical Questions (HyDE Approach) ‚ùì\n",
    "Overview üîé\n",
    "Generating hypothetical questions to improve alignment between queries and data.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "Create hypothetical questions that point to relevant locations in the data, enhancing query-data matching.\n",
    "\n",
    "9. Choose Chunk Size üìè\n",
    "Overview üîé\n",
    "Selecting an appropriate fixed size for text chunks to balance context preservation and retrieval efficiency.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "Experiment with different chunk sizes to find the optimal balance between preserving context and maintaining retrieval speed for your specific use case.\n",
    "\n",
    "10. Semantic Chunking üß†\n",
    "Overview üîé\n",
    "Dividing documents based on semantic coherence rather than fixed sizes.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "Use NLP techniques to identify topic boundaries or coherent sections within documents for more meaningful retrieval units.\n",
    "\n",
    "11. Contextual Compression üóúÔ∏è\n",
    "Overview üîé\n",
    "Compressing retrieved information while preserving query-relevant content.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "Use an LLM to compress or summarize retrieved chunks, preserving key information relevant to the query.\n",
    "\n",
    "12. Explainable Retrieval üîç\n",
    "Overview üîé\n",
    "Providing transparency in the retrieval process to enhance user trust and system refinement.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "Explain why certain pieces of information were retrieved and how they relate to the query.\n",
    "\n",
    "13. Retrieval with Feedback Loops üîÅ\n",
    "Overview üîé\n",
    "Implementing mechanisms to learn from user interactions and improve future retrievals.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "Collect and utilize user feedback on the relevance and quality of retrieved documents and generated responses to fine-tune retrieval and ranking models.\n",
    "\n",
    "14. Adaptive Retrieval üéØ\n",
    "Overview üîé\n",
    "Dynamically adjusting retrieval strategies based on query types and user contexts.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "Classify queries into different categories and use tailored retrieval strategies for each, considering user context and preferences.\n",
    "\n",
    "15. Iterative Retrieval üîÑ\n",
    "Overview üîé\n",
    "Performing multiple rounds of retrieval to refine and enhance result quality.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "Use the LLM to analyze initial results and generate follow-up queries to fill in gaps or clarify information.\n",
    "\n",
    "16. Ensemble Retrieval üé≠\n",
    "Overview üîé\n",
    "Combining multiple retrieval models or techniques for more robust and accurate results.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "Apply different embedding models or retrieval algorithms and use voting or weighting mechanisms to determine the final set of retrieved documents.\n",
    "\n",
    "17. Knowledge Graph Integration (Graph RAG)üï∏Ô∏è\n",
    "Overview üîé\n",
    "Incorporating structured data from knowledge graphs to enrich context and improve retrieval.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "Retrieve entities and their relationships from a knowledge graph relevant to the query, combining this structured data with unstructured text for more informative responses.\n",
    "\n",
    "18. Multi-modal Retrieval üìΩÔ∏è\n",
    "Overview üîé\n",
    "Extending RAG capabilities to handle diverse data types for richer responses.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "Integrate models that can retrieve and understand different data modalities, combining insights from text, images, and videos.\n",
    "\n",
    "19. RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval üå≥\n",
    "Overview üîé\n",
    "Implementing a recursive approach to process and organize retrieved information in a tree structure.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "Use abstractive summarization to recursively process and summarize retrieved documents, organizing the information in a tree structure for hierarchical context.\n",
    "\n",
    "20. Self RAG üîÅ\n",
    "Overview üîé\n",
    "A dynamic approach that combines retrieval-based and generation-based methods, adaptively deciding whether to use retrieved information and how to best utilize it in generating responses.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "‚Ä¢ Implement a multi-step process including retrieval decision, document retrieval, relevance evaluation, response generation, support assessment, and utility evaluation to produce accurate, relevant, and useful outputs.\n",
    "\n",
    "21. Corrective RAG üîß\n",
    "Overview üîé\n",
    "A sophisticated RAG approach that dynamically evaluates and corrects the retrieval process, combining vector databases, web search, and language models for highly accurate and context-aware responses.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "‚Ä¢ Integrate Retrieval Evaluator, Knowledge Refinement, Web Search Query Rewriter, and Response Generator components to create a system that adapts its information sourcing strategy based on relevance scores and combines multiple sources when necessary.\n",
    "\n",
    "22. Document Augmentation through Question Generation for Enhanced Retrieval\n",
    "Overview üîé\n",
    "This implementation demonstrates a text augmentation technique that leverages additional question generation to improve document retrieval within a vector database. By generating and incorporating various questions related to each text fragment, the system enhances the standard retrieval process, thus increasing the likelihood of finding relevant documents that can be utilized as context for generative question answering.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "Use an LLM to augment text dataset with all possible questions that can be asked to each document.\n",
    "\n",
    "üåü Special Advanced Technique üåü\n",
    "23. Sophisticated Controllable Agent for Complex RAG Tasks ü§ñ\n",
    "Overview üîé\n",
    "An advanced RAG solution designed to tackle complex questions that simple semantic similarity-based retrieval cannot solve. This approach uses a sophisticated deterministic graph as the \"brain\" üß† of a highly controllable autonomous agent, capable of answering non-trivial questions from your own data.\n",
    "\n",
    "Implementation üõ†Ô∏è\n",
    "‚Ä¢ Implement a multi-step process involving question anonymization, high-level planning, task breakdown, adaptive information retrieval and question answering, continuous re-planning, and rigorous answer verification to ensure grounded and accurate responses.\n",
    "'''\n",
    "response = model.generate_content(\n",
    "    f\"Summarize: {input_text}, i want a very brief list of the techniques and a obsidian link to the specific technique, the obsidian link should start with RAG - \"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is a research paper about a new approach to reinforcement learning in Large Language Models (LLMs) called Reflexion. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "**Problem:** Traditional reinforcement learning methods are not very effective for LLMs. They require a lot of data and computation, which is not feasible for massive models. Existing methods often rely on in-context examples, limiting the agent's ability to learn and generalize.\n",
      "\n",
      "**Proposed Solution: Reflexion**\n",
      "\n",
      "* **Core Idea:**  Enable LLMs to learn from their mistakes through linguistic feedback and self-reflection. \n",
      "* **Process:**\n",
      "    1. **Agent takes action:** The LLM-based agent interacts with an environment (e.g., code execution, text-based game, question answering).\n",
      "    2. **Evaluation:**  The agent's performance is evaluated (either through external feedback or internal self-assessment).\n",
      "    3. **Verbal Self-Reflection:** The agent generates a textual summary reflecting on its performance and identifying areas for improvement (e.g., \"I made a mistake assuming X. I should have done Y instead\").\n",
      "    4. **Memory:** This self-reflection is stored in the agent's memory to guide future decisions. \n",
      "* **Advantages of Reflexion:**\n",
      "    * Lightweight: Doesn't require expensive model fine-tuning.\n",
      "    * Handles nuanced feedback: Allows for more specific feedback compared to simple scalar rewards.\n",
      "    * Interpretable memory: Creates a more transparent and understandable record of the agent's learning process.\n",
      "    * Explicit action hints:  Provides the agent with clearer guidance for improvement.\n",
      "\n",
      "**Experiments and Results:**\n",
      "\n",
      "* The paper tests Reflexion on a range of tasks:\n",
      "    * Decision-making (AlfWorld): Agent navigates text-based environments.\n",
      "    * Reasoning (HotpotQA): Agent answers multi-hop questions using Wikipedia data.\n",
      "    * Programming (HumanEval, MBPP, LeetcodeHardGym): Agent generates code. \n",
      "* Reflexion significantly outperforms baseline models across these tasks, demonstrating its effectiveness in learning and improvement.\n",
      "\n",
      "**Overall:** This paper introduces a promising direction for making LLMs more effective learners through the power of language and self-reflection. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "\n",
    "vertexai.init(project=project_id, location=location)\n",
    "\n",
    "model = GenerativeModel(model_name='gemini-1.5-pro-001')\n",
    "\n",
    "# Load PDF into Part object\n",
    "pdf_file_path = \"media/reflextion.pdf\"\n",
    "\n",
    "# Read the PDF file as bytes\n",
    "with open(pdf_file_path, \"rb\") as file:\n",
    "    pdf_bytes = file.read()\n",
    "\n",
    "# Create Part from bytes data\n",
    "pdf = Part.from_data(pdf_bytes, mime_type=\"application/pdf\")\n",
    "\n",
    "prompt = \"What is shown in this document?\"\n",
    "\n",
    "contents = [pdf, prompt]\n",
    "\n",
    "try:\n",
    "    response = model.generate_content(contents)\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    # If available, print more detailed error information\n",
    "    if hasattr(e, 'details'):\n",
    "        print(f\"Error details: {e.details}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a plate with blueberry scones, blueberries, two cups of coffee, and a silver spoon with \"let's jam\" engraved on it. There are also pink peonies in the image.\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "\n",
    "# TODO(developer): Update and un-comment below line\n",
    "# project_id = \"PROJECT_ID\"\n",
    "\n",
    "vertexai.init(project=project_id, location=location)\n",
    "\n",
    "model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
    "\n",
    "response = model.generate_content(\n",
    "    [\n",
    "        Part.from_uri(\n",
    "            \"gs://cloud-samples-data/generative-ai/image/scones.jpg\",\n",
    "            mime_type=\"image/jpeg\",\n",
    "        ),\n",
    "        \"What is shown in this image?\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "from gemini.prompt import audio_transcription\n",
    "\n",
    "# TODO(developer): Update and un-comment below lines\n",
    "# project_id = \"PROJECT_ID\"\n",
    "\n",
    "vertexai.init(project=project_id, location=location)\n",
    "\n",
    "model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
    "\n",
    "prompt = audio_transcription\n",
    "audio_file_path = \"media/filename.mp3\"\n",
    "\n",
    "# Read the audio file in binary mode\n",
    "with open(audio_file_path, \"rb\") as audio_file:\n",
    "    audio_bytes = audio_file.read()\n",
    "\n",
    "audio_part = Part.from_data(audio_bytes, mime_type=\"audio/mp3\")\n",
    "\n",
    "contents = [audio_part, prompt]\n",
    "\n",
    "# Counts tokens\n",
    "print(model.count_tokens(contents))\n",
    "response = model.generate_content(contents)\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here is a breakdown of six AI agent architectures for large language models, based on the video you described.\n",
      "\n",
      "## Agent Architectures for LLMs: A Breakdown\n",
      "\n",
      "This video explores different architectural approaches to building AI agents powered by large language models (LLMs). Each architecture offers unique mechanisms for reasoning, planning, and executing tasks, with varying levels of complexity and efficiency.\n",
      "\n",
      "### 1. Basic Reflection\n",
      "\n",
      "**Description:**\n",
      "* User query triggers an initial response generated by an LLM (e.g., an essay writer).\n",
      "* This response is then fed into a second \"reflection\" LLM, which acts as an evaluator (e.g., an essay grader).\n",
      "* The reflection LLM generates critiques and ideas for improvements.\n",
      "* These critiques are fed back into the original LLM to generate a revised draft.\n",
      "* This loop repeats multiple times until a satisfactory output is achieved.\n",
      "\n",
      "**Pros:**\n",
      "* Simple and intuitive to implement.\n",
      "* Can iteratively improve the quality of generated outputs.\n",
      "\n",
      "**Cons:**\n",
      "* Can be computationally expensive due to repeated regeneration of the entire response.\n",
      "* Lack of external tool integration limits its capabilities.\n",
      "\n",
      "### 2. Reflexion Actor\n",
      "\n",
      "**Description:**\n",
      "* Builds on the basic reflection concept, introducing tool use.\n",
      "* User query triggers an initial response and self-critique, along with suggested tool queries.\n",
      "* These tool queries are executed (e.g., web searches for more information).\n",
      "* Original response, self-reflection, and additional context from the executed tools are sent to a \"reviser\" LLM.\n",
      "* The reviser updates the answer, generates a new self-reflection, and creates new suggested tool queries.\n",
      "* This loop repeats until a satisfactory output is generated.\n",
      "\n",
      "**Pros:**\n",
      "* Enhances the agent's capabilities by incorporating external tools.\n",
      "* Separates tool execution from the revision process, potentially improving efficiency.\n",
      "\n",
      "**Cons:**\n",
      "* Still computationally intensive, especially for tasks requiring extensive tool usage or dealing with large text inputs.\n",
      "* Prone to generating unverified claims or information, as seen in the LangSmith trace example.\n",
      "\n",
      "### 3. Language Agent Tree Search (LATS)\n",
      "\n",
      "**Description:**\n",
      "* Employs a tree search algorithm, where LLMs are used as agents, value functions, and optimizers.\n",
      "* Initial response is generated as a starting root node (either an answer or a tool execution).\n",
      "* A reflection prompt generates a reflection on the output, a score for the output, and determines if a solution is found.\n",
      "* If a solution is not found, additional candidates are generated with the context of the prior output and reflection, expanding the tree.\n",
      "* The process repeats until a solution is found or a maximum search depth is reached.\n",
      "\n",
      "**Pros:**\n",
      "* Explores a wider range of possibilities through tree search, potentially leading to better solutions.\n",
      "* Leverages the power of Monte Carlo tree search for efficient exploration and optimization.\n",
      "\n",
      "**Cons:**\n",
      "* Highly abstract and complex to implement.\n",
      "* Scoring and selection of candidate nodes can be unreliable due to the inherent indeterministic nature of LLMs.\n",
      "\n",
      "### 4. Plan-And-Execute\n",
      "\n",
      "**Description:**\n",
      "* Focuses on planning and decomposing a task into sub-tasks before execution.\n",
      "* User query triggers an initial planning prompt, which creates a step-by-step plan for completing the query.\n",
      "* The first step of the plan is executed by an agent, which either generates a response or executes a tool.\n",
      "* The original query, original plan, and past step output are then given to a re-planner prompt, which updates the plan or returns the output to the user.\n",
      "* This process repeats until the re-planner deems the task complete.\n",
      "\n",
      "**Pros:**\n",
      "* Improves efficiency by decomposing complex tasks into manageable sub-tasks.\n",
      "* Offers more transparency and control over the agent's reasoning process.\n",
      "\n",
      "**Cons:**\n",
      "* Higher token usage compared to direct reflection approaches.\n",
      "* Still relies heavily on LLM's planning and reasoning capabilities, which can be prone to errors.\n",
      "\n",
      "### 5. Reasoning without Observation (ReWOO)\n",
      "\n",
      "**Description:**\n",
      "* Optimizes the plan-and-execute approach by decoupling reasoning from observation.\n",
      "* Combines a multi-step planner with variable substitution for efficient tool use, aiming to reduce token consumption and execution time.\n",
      "* The planner generates a complete task list with placeholder variables, considering dependencies between tasks.\n",
      "* All tasks are executed in a single step, with interdependencies resolved internally.\n",
      "* The final output is then generated based on the collected \"evidence\" from the executed tasks.\n",
      "\n",
      "**Pros:**\n",
      "* Significantly reduces processing time by executing all tasks in a single step, maximizing parallel processing.\n",
      "* Efficiently uses variable substitution for tool outputs, minimizing redundant calculations.\n",
      "\n",
      "**Cons:**\n",
      "* Requires careful planning and understanding of task dependencies to ensure correct execution.\n",
      "* Can be challenging to implement and debug, given its intricate nature.\n",
      "\n",
      "### 6. LLM Compiler\n",
      "\n",
      "**Description:**\n",
      "* Further optimizes the ReWOO approach by leveraging concepts from compiler design.\n",
      "* The LLM acts as a compiler, parsing the user query and generating a Directed Acyclic Graph (DAG) of tasks with interdependencies.\n",
      "* The DAG allows for parallel execution of independent tasks, further reducing processing time.\n",
      "* Once all tasks are executed, the results are combined and delivered to the user, bypassing the need for individual task observations or reflections.\n",
      "\n",
      "**Pros:**\n",
      "* Achieves significant speed and efficiency gains through parallel execution and optimized task orchestration.\n",
      "* The use of DAGs for task management enhances clarity and control over the workflow.\n",
      "\n",
      "**Cons:**\n",
      "* Requires a deep understanding of compiler design principles for effective implementation.\n",
      "* May require significant engineering effort to integrate with existing LLM frameworks.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "This breakdown showcases the diverse landscape of AI agent architectures, each with its own strengths and weaknesses. The choice of architecture ultimately depends on the specific application, the desired level of efficiency, and the complexity of the task at hand. As research in this field progresses, we can expect to see further innovation and optimization, pushing the boundaries of what AI agents can achieve.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "\n",
    "vertexai.init(project=project_id, location=location)\n",
    "\n",
    "model = GenerativeModel(\"gemini-1.5-pro-001\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "I provide you a video, please provide me a summary of the video. The video is about the architecture of ai agents, for each architecture please provide a description of the architecture and the pros and cons of the architecture and all details you thing are important. The output should be in markdown format.\n",
    "\"\"\"\n",
    "\n",
    "video_file_path = \"media/ai_agents_architecture.mp4\"\n",
    "\n",
    "# Leggi il file video in modalit√† binaria\n",
    "with open(video_file_path, \"rb\") as video_file:\n",
    "    video_bytes = video_file.read()\n",
    "\n",
    "video_part = Part.from_data(video_bytes, mime_type=\"video/mp4\")\n",
    "\n",
    "contents = [video_part, prompt]\n",
    "\n",
    "response = model.generate_content(contents)\n",
    "print(response.text)\n",
    "# save the markdown file\n",
    "with open(\"media/ai_agents_architecture_summary.md\", \"w\") as f:\n",
    "    f.write(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv_env)",
   "language": "python",
   "name": "my_uv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
