{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New working directory: /home/fd/repo/not-work-repo/LLM-AI-experiments\n"
     ]
    }
   ],
   "source": [
    "# modifica al path del progetto\n",
    "import os\n",
    "# Modifica il percorso di lavoro se necessario\n",
    "os.chdir('..')\n",
    "# Stampa di nuovo il percorso di lavoro per conferma\n",
    "print(\"New working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "project_id = os.getenv(\"gemini_project_id\")\n",
    "location = os.getenv(\"gemini_location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How use Class Part\n",
    "```\n",
    "text_part = Part.from_text(\"Why is sky blue?\")\n",
    "image_part = Part.from_image(Image.load_from_file(\"image.jpg\"))\n",
    "video_part = Part.from_uri(uri=\"gs://.../video.mp4\", mime_type=\"video/mp4\")\n",
    "function_response_part = Part.from_function_response(\n",
    "    name=\"get_current_weather\",\n",
    "    response={\n",
    "        \"content\": {\"weather_there\": \"super nice\"},\n",
    "    }\n",
    ")\n",
    "\n",
    "response1 = model.generate_content([text_part, image_part])\n",
    "response2 = model.generate_content(video_part)\n",
    "response3 = chat.send_message(function_response_part)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available methods to create a Part instance:\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_from_gapic', '_image', 'file_data', 'from_data', 'from_dict', 'from_function_response', 'from_image', 'from_text', 'from_uri', 'function_call', 'function_response', 'inline_data', 'mime_type', 'text', 'to_dict']\n",
      "\n",
      "\n",
      "Available methods to create a Part instance and their parameters:\n",
      "_from_gapic(raw_part: raw_part: google.cloud.aiplatform_v1beta1.types.content.Part)\n",
      "from_data(data: data: bytes, mime_type: mime_type: str)\n",
      "from_dict(part_dict: part_dict: Dict[str, Any])\n",
      "from_function_response(name: name: str, response: response: Dict[str, Any])\n",
      "from_image(image: image: 'Image')\n",
      "from_text(text: text: str)\n",
      "from_uri(uri: uri: str, mime_type: mime_type: str)\n",
      "to_dict(self: self)\n"
     ]
    }
   ],
   "source": [
    "# To explore all the possible methods to create a Part instance, we can list the available class methods of the Part class.\n",
    "# This will include methods like from_text, from_image, from_uri, and from_function_response.\n",
    "# Additionally, we can inspect the parameters required by each method using the inspect module.\n",
    "\n",
    "# We can use the dir() function on the Part class itself to see all the class methods.\n",
    "print(\"Available methods to create a Part instance:\")\n",
    "print([method for method in dir(Part)])\n",
    "\n",
    "import inspect\n",
    "\n",
    "print(\"\\n\")\n",
    "# Display available methods to create a Part instance along with their parameters in a more readable format\n",
    "print(\"Available methods to create a Part instance and their parameters:\")\n",
    "for method in dir(Part):\n",
    "    if callable(getattr(Part, method)) and not method.startswith(\"__\"):\n",
    "        params = inspect.signature(getattr(Part, method)).parameters\n",
    "        param_list = ', '.join(f\"{name}: {param}\" for name, param in params.items())\n",
    "        print(f\"{method}({param_list})\")\n",
    "\n",
    "# Additionally, check if 'file_data' method exists and print its parameters if it does\n",
    "if hasattr(Part, 'file_data') and callable(getattr(Part, 'file_data')):\n",
    "    file_data_params = inspect.signature(getattr(Part, 'file_data')).parameters\n",
    "    file_data_param_list = ', '.join(f\"{name}: {param}\" for name, param in file_data_params.items())\n",
    "    print(f\"file_data({file_data_param_list})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Advanced RAG Techniques: Obsidian Links for Deeper Exploration \n",
      "\n",
      "Here's your concise list with Obsidian links for easy access: \n",
      "\n",
      "**Core Techniques:**\n",
      "\n",
      "- **[[RAG - Simple RAG|Simple RAG]]**: Basic retrieval & integration. \n",
      "- **[[RAG - Context Enrichment Techniques|Context Enrichment]]**: Enhancing retrieval with neighboring sentences.\n",
      "- **[[RAG - Multi-faceted Filtering|Multi-faceted Filtering]]**: Refining results with metadata, similarity, content, and diversity filters.\n",
      "- **[[RAG - Fusion Retrieval|Fusion Retrieval]]**: Combining retrieval methods for comprehensive results. \n",
      "- **[[RAG - Intelligent Reranking|Intelligent Reranking]]**: Improving relevance ranking with advanced scoring.\n",
      "\n",
      "**Advanced Retrieval & Optimization:**\n",
      "\n",
      "- **[[RAG - Query Transformations|Query Transformations]]**: Modifying queries for better retrieval.\n",
      "- **[[RAG - Hierarchical Indices|Hierarchical Indices]]**: Multi-tiered systems for efficient navigation. \n",
      "- **[[RAG - Hypothetical Questions (HyDE Approach)|Hypothetical Questions (HyDE)]]**: Generating questions to improve query-data alignment.\n",
      "- **[[RAG - Choose Chunk Size|Choose Chunk Size]]**: Balancing context and retrieval speed.\n",
      "- **[[RAG - Semantic Chunking|Semantic Chunking]]**: Dividing text based on meaning. \n",
      "- **[[RAG - Contextual Compression|Contextual Compression]]**: Summarizing while preserving relevance. \n",
      "- **[[RAG - Explainable Retrieval|Explainable Retrieval]]**: Providing transparency in the process.\n",
      "- **[[RAG - Retrieval with Feedback Loops|Retrieval with Feedback Loops]]**:  Learning from user interactions. \n",
      "- **[[RAG - Adaptive Retrieval|Adaptive Retrieval]]**: Adjusting strategies based on query and context.\n",
      "\n",
      "**Cutting-Edge Approaches:**\n",
      "\n",
      "- **[[RAG - Iterative Retrieval|Iterative Retrieval]]**: Multiple retrieval rounds for refinement. \n",
      "- **[[RAG - Ensemble Retrieval|Ensemble Retrieval]]**: Combining models for robustness. \n",
      "- **[[RAG - Knowledge Graph Integration (Graph RAG)|Knowledge Graph Integration (Graph RAG)]]**: Enriching context with structured data.\n",
      "- **[[RAG - Multi-modal Retrieval|Multi-modal Retrieval]]**: Handling diverse data types.\n",
      "- **[[RAG - RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval|RAPTOR]]**:  Recursive processing for tree-structured information. \n",
      "- **[[RAG - Self RAG|Self RAG]]**: Dynamically choosing between retrieval and generation. \n",
      "- **[[RAG - Corrective RAG|Corrective RAG]]**: Evaluating and correcting retrieval on the fly.\n",
      "- **[[RAG - Document Augmentation through Question Generation for Enhanced Retrieval|Document Augmentation]]**: Enhancing retrieval with generated questions.\n",
      "- **[[RAG - Sophisticated Controllable Agent for Complex RAG Tasks|Sophisticated Controllable Agent]]**: Handling complex tasks with a deterministic graph-based agent. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "vertexai.init(project=project_id, location=\"europe-west1\")\n",
    "\n",
    "model = GenerativeModel(\"gemini-1.5-pro-001\")\n",
    "\n",
    "input_text = '''\n",
    "'''\n",
    "response = model.generate_content(\n",
    "    f\"Summarize: {input_text}, i want a very brief list of the techniques and a obsidian link to the specific technique, the obsidian link should start with RAG - \"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is a research paper about a new approach to reinforcement learning in Large Language Models (LLMs) called Reflexion. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "**Problem:** Traditional reinforcement learning methods are not very effective for LLMs. They require a lot of data and computation, which is not feasible for massive models. Existing methods often rely on in-context examples, limiting the agent's ability to learn and generalize.\n",
      "\n",
      "**Proposed Solution: Reflexion**\n",
      "\n",
      "* **Core Idea:**  Enable LLMs to learn from their mistakes through linguistic feedback and self-reflection. \n",
      "* **Process:**\n",
      "    1. **Agent takes action:** The LLM-based agent interacts with an environment (e.g., code execution, text-based game, question answering).\n",
      "    2. **Evaluation:**  The agent's performance is evaluated (either through external feedback or internal self-assessment).\n",
      "    3. **Verbal Self-Reflection:** The agent generates a textual summary reflecting on its performance and identifying areas for improvement (e.g., \"I made a mistake assuming X. I should have done Y instead\").\n",
      "    4. **Memory:** This self-reflection is stored in the agent's memory to guide future decisions. \n",
      "* **Advantages of Reflexion:**\n",
      "    * Lightweight: Doesn't require expensive model fine-tuning.\n",
      "    * Handles nuanced feedback: Allows for more specific feedback compared to simple scalar rewards.\n",
      "    * Interpretable memory: Creates a more transparent and understandable record of the agent's learning process.\n",
      "    * Explicit action hints:  Provides the agent with clearer guidance for improvement.\n",
      "\n",
      "**Experiments and Results:**\n",
      "\n",
      "* The paper tests Reflexion on a range of tasks:\n",
      "    * Decision-making (AlfWorld): Agent navigates text-based environments.\n",
      "    * Reasoning (HotpotQA): Agent answers multi-hop questions using Wikipedia data.\n",
      "    * Programming (HumanEval, MBPP, LeetcodeHardGym): Agent generates code. \n",
      "* Reflexion significantly outperforms baseline models across these tasks, demonstrating its effectiveness in learning and improvement.\n",
      "\n",
      "**Overall:** This paper introduces a promising direction for making LLMs more effective learners through the power of language and self-reflection. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "\n",
    "vertexai.init(project=project_id, location=location)\n",
    "\n",
    "model = GenerativeModel(model_name='gemini-1.5-pro-001')\n",
    "\n",
    "# Load PDF into Part object\n",
    "pdf_file_path = \"media/reflextion.pdf\"\n",
    "\n",
    "# Read the PDF file as bytes\n",
    "with open(pdf_file_path, \"rb\") as file:\n",
    "    pdf_bytes = file.read()\n",
    "\n",
    "# Create Part from bytes data\n",
    "pdf = Part.from_data(pdf_bytes, mime_type=\"application/pdf\")\n",
    "\n",
    "prompt = \"What is shown in this document?\"\n",
    "\n",
    "contents = [pdf, prompt]\n",
    "\n",
    "try:\n",
    "    response = model.generate_content(contents)\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    # If available, print more detailed error information\n",
    "    if hasattr(e, 'details'):\n",
    "        print(f\"Error details: {e.details}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a plate with blueberry scones, blueberries, two cups of coffee, and a silver spoon with \"let's jam\" engraved on it. There are also pink peonies in the image.\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "\n",
    "# TODO(developer): Update and un-comment below line\n",
    "# project_id = \"PROJECT_ID\"\n",
    "\n",
    "vertexai.init(project=project_id, location=location)\n",
    "\n",
    "model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
    "\n",
    "response = model.generate_content(\n",
    "    [\n",
    "        Part.from_uri(\n",
    "            \"gs://cloud-samples-data/generative-ai/image/scones.jpg\",\n",
    "            mime_type=\"image/jpeg\",\n",
    "        ),\n",
    "        \"What is shown in this image?\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "from gemini.prompt import audio_transcription\n",
    "\n",
    "# TODO(developer): Update and un-comment below lines\n",
    "# project_id = \"PROJECT_ID\"\n",
    "\n",
    "vertexai.init(project=project_id, location=location)\n",
    "\n",
    "model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
    "\n",
    "prompt = audio_transcription\n",
    "audio_file_path = \"media/filename.mp3\"\n",
    "\n",
    "# Read the audio file in binary mode\n",
    "with open(audio_file_path, \"rb\") as audio_file:\n",
    "    audio_bytes = audio_file.read()\n",
    "\n",
    "audio_part = Part.from_data(audio_bytes, mime_type=\"audio/mp3\")\n",
    "\n",
    "contents = [audio_part, prompt]\n",
    "\n",
    "# Counts tokens\n",
    "print(model.count_tokens(contents))\n",
    "response = model.generate_content(contents)\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here is a breakdown of six AI agent architectures for large language models, based on the video you described.\n",
      "\n",
      "## Agent Architectures for LLMs: A Breakdown\n",
      "\n",
      "This video explores different architectural approaches to building AI agents powered by large language models (LLMs). Each architecture offers unique mechanisms for reasoning, planning, and executing tasks, with varying levels of complexity and efficiency.\n",
      "\n",
      "### 1. Basic Reflection\n",
      "\n",
      "**Description:**\n",
      "* User query triggers an initial response generated by an LLM (e.g., an essay writer).\n",
      "* This response is then fed into a second \"reflection\" LLM, which acts as an evaluator (e.g., an essay grader).\n",
      "* The reflection LLM generates critiques and ideas for improvements.\n",
      "* These critiques are fed back into the original LLM to generate a revised draft.\n",
      "* This loop repeats multiple times until a satisfactory output is achieved.\n",
      "\n",
      "**Pros:**\n",
      "* Simple and intuitive to implement.\n",
      "* Can iteratively improve the quality of generated outputs.\n",
      "\n",
      "**Cons:**\n",
      "* Can be computationally expensive due to repeated regeneration of the entire response.\n",
      "* Lack of external tool integration limits its capabilities.\n",
      "\n",
      "### 2. Reflexion Actor\n",
      "\n",
      "**Description:**\n",
      "* Builds on the basic reflection concept, introducing tool use.\n",
      "* User query triggers an initial response and self-critique, along with suggested tool queries.\n",
      "* These tool queries are executed (e.g., web searches for more information).\n",
      "* Original response, self-reflection, and additional context from the executed tools are sent to a \"reviser\" LLM.\n",
      "* The reviser updates the answer, generates a new self-reflection, and creates new suggested tool queries.\n",
      "* This loop repeats until a satisfactory output is generated.\n",
      "\n",
      "**Pros:**\n",
      "* Enhances the agent's capabilities by incorporating external tools.\n",
      "* Separates tool execution from the revision process, potentially improving efficiency.\n",
      "\n",
      "**Cons:**\n",
      "* Still computationally intensive, especially for tasks requiring extensive tool usage or dealing with large text inputs.\n",
      "* Prone to generating unverified claims or information, as seen in the LangSmith trace example.\n",
      "\n",
      "### 3. Language Agent Tree Search (LATS)\n",
      "\n",
      "**Description:**\n",
      "* Employs a tree search algorithm, where LLMs are used as agents, value functions, and optimizers.\n",
      "* Initial response is generated as a starting root node (either an answer or a tool execution).\n",
      "* A reflection prompt generates a reflection on the output, a score for the output, and determines if a solution is found.\n",
      "* If a solution is not found, additional candidates are generated with the context of the prior output and reflection, expanding the tree.\n",
      "* The process repeats until a solution is found or a maximum search depth is reached.\n",
      "\n",
      "**Pros:**\n",
      "* Explores a wider range of possibilities through tree search, potentially leading to better solutions.\n",
      "* Leverages the power of Monte Carlo tree search for efficient exploration and optimization.\n",
      "\n",
      "**Cons:**\n",
      "* Highly abstract and complex to implement.\n",
      "* Scoring and selection of candidate nodes can be unreliable due to the inherent indeterministic nature of LLMs.\n",
      "\n",
      "### 4. Plan-And-Execute\n",
      "\n",
      "**Description:**\n",
      "* Focuses on planning and decomposing a task into sub-tasks before execution.\n",
      "* User query triggers an initial planning prompt, which creates a step-by-step plan for completing the query.\n",
      "* The first step of the plan is executed by an agent, which either generates a response or executes a tool.\n",
      "* The original query, original plan, and past step output are then given to a re-planner prompt, which updates the plan or returns the output to the user.\n",
      "* This process repeats until the re-planner deems the task complete.\n",
      "\n",
      "**Pros:**\n",
      "* Improves efficiency by decomposing complex tasks into manageable sub-tasks.\n",
      "* Offers more transparency and control over the agent's reasoning process.\n",
      "\n",
      "**Cons:**\n",
      "* Higher token usage compared to direct reflection approaches.\n",
      "* Still relies heavily on LLM's planning and reasoning capabilities, which can be prone to errors.\n",
      "\n",
      "### 5. Reasoning without Observation (ReWOO)\n",
      "\n",
      "**Description:**\n",
      "* Optimizes the plan-and-execute approach by decoupling reasoning from observation.\n",
      "* Combines a multi-step planner with variable substitution for efficient tool use, aiming to reduce token consumption and execution time.\n",
      "* The planner generates a complete task list with placeholder variables, considering dependencies between tasks.\n",
      "* All tasks are executed in a single step, with interdependencies resolved internally.\n",
      "* The final output is then generated based on the collected \"evidence\" from the executed tasks.\n",
      "\n",
      "**Pros:**\n",
      "* Significantly reduces processing time by executing all tasks in a single step, maximizing parallel processing.\n",
      "* Efficiently uses variable substitution for tool outputs, minimizing redundant calculations.\n",
      "\n",
      "**Cons:**\n",
      "* Requires careful planning and understanding of task dependencies to ensure correct execution.\n",
      "* Can be challenging to implement and debug, given its intricate nature.\n",
      "\n",
      "### 6. LLM Compiler\n",
      "\n",
      "**Description:**\n",
      "* Further optimizes the ReWOO approach by leveraging concepts from compiler design.\n",
      "* The LLM acts as a compiler, parsing the user query and generating a Directed Acyclic Graph (DAG) of tasks with interdependencies.\n",
      "* The DAG allows for parallel execution of independent tasks, further reducing processing time.\n",
      "* Once all tasks are executed, the results are combined and delivered to the user, bypassing the need for individual task observations or reflections.\n",
      "\n",
      "**Pros:**\n",
      "* Achieves significant speed and efficiency gains through parallel execution and optimized task orchestration.\n",
      "* The use of DAGs for task management enhances clarity and control over the workflow.\n",
      "\n",
      "**Cons:**\n",
      "* Requires a deep understanding of compiler design principles for effective implementation.\n",
      "* May require significant engineering effort to integrate with existing LLM frameworks.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "This breakdown showcases the diverse landscape of AI agent architectures, each with its own strengths and weaknesses. The choice of architecture ultimately depends on the specific application, the desired level of efficiency, and the complexity of the task at hand. As research in this field progresses, we can expect to see further innovation and optimization, pushing the boundaries of what AI agents can achieve.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "\n",
    "vertexai.init(project=project_id, location=location)\n",
    "\n",
    "model = GenerativeModel(\"gemini-1.5-pro-001\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "I provide you a video, please provide me a summary of the video. The video is about the architecture of ai agents, for each architecture please provide a description of the architecture and the pros and cons of the architecture and all details you thing are important. The output should be in markdown format.\n",
    "\"\"\"\n",
    "\n",
    "video_file_path = \"media/ai_agents_architecture.mp4\"\n",
    "\n",
    "# Leggi il file video in modalità binaria\n",
    "with open(video_file_path, \"rb\") as video_file:\n",
    "    video_bytes = video_file.read()\n",
    "\n",
    "video_part = Part.from_data(video_bytes, mime_type=\"video/mp4\")\n",
    "\n",
    "contents = [video_part, prompt]\n",
    "\n",
    "response = model.generate_content(contents)\n",
    "print(response.text)\n",
    "# save the markdown file\n",
    "with open(\"media/ai_agents_architecture_summary.md\", \"w\") as f:\n",
    "    f.write(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv_env)",
   "language": "python",
   "name": "my_uv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
