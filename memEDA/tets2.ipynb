{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from typing import TypedDict, Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "import chainlit as cl\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LangSmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Import the prompts dictionary\n",
    "from libs.prompts import prompts\n",
    "\n",
    "# Define the get_prompt function to retrieve prompts from the dictionary\n",
    "def get_prompt(prompts, name):\n",
    "    return prompts.get(name, f\"Prompt with name '{name}' not found.\")\n",
    "\n",
    "# Use the get_prompt function to retrieve the desired prompts\n",
    "system_prompt_sentinel_EDA = get_prompt(prompts, \"system_prompt_memory_sentinel_EDA\")\n",
    "system_prompt_memory_manager = get_prompt(prompts, \"system_prompt_memory_manager_EDA\")\n",
    "system_prompt_query_answering = get_prompt(prompts, \"system_prompt_query_answering\")\n",
    "\n",
    "# Set up Agent: Memory Sentinel\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt_sentinel_EDA),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Remember, only respond with TRUE or FALSE. Do not provide any other information.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    streaming=True,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "sentinel_runnable = {\"messages\": RunnablePassthrough()} | prompt | llm\n",
    "\n",
    "# Set up Agent: Memory Manager\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import StructuredTool\n",
    "from enum import Enum\n",
    "from typing import Optional\n",
    "\n",
    "class Category(str, Enum):\n",
    "    KEY_VARIABLES = \"key_variables\"\n",
    "    CURRENT_EDA_STATUS = \"current_eda_status\"\n",
    "    NEXT_STEPS = \"next_steps\"\n",
    "    DATA_QUALITY_ISSUES = \"data_quality_issues\"\n",
    "    STATISTICAL_SUMMARIES = \"statistical_summaries\"\n",
    "    INSIGHTS = \"insights\"\n",
    "    EXTERNAL_FACTORS = \"external_factors\"\n",
    "    DATA_SOURCES = \"data_sources\"\n",
    "\n",
    "class Action(str, Enum):\n",
    "    ADD = \"add\"\n",
    "    UPDATE = \"update\"\n",
    "    DELETE = \"delete\"\n",
    "\n",
    "class AddKnowledge(BaseModel):\n",
    "    knowledge: str = Field(\n",
    "        ...,\n",
    "        description=\"Structured information about the dataset or EDA progress to be saved or updated\",\n",
    "    )\n",
    "    knowledge_old: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"If updating or deleting, the complete, exact phrase that needs to be modified\",\n",
    "    )\n",
    "    category: Category = Field(\n",
    "        ..., description=\"Category that this information belongs to\"\n",
    "    )\n",
    "    action: Action = Field(\n",
    "        ...,\n",
    "        description=\"Whether this information is adding a new record, updating a record, or deleting a record\",\n",
    "    )\n",
    "\n",
    "import json\n",
    "\n",
    "MEMORY_FILE = \"memories.json\"\n",
    "\n",
    "def modify_knowledge(\n",
    "    knowledge: str,\n",
    "    category: Category,\n",
    "    action: Action,\n",
    "    knowledge_old: str = \"\",\n",
    ") -> dict:\n",
    "    print(f\"Modifying Dataset Description: {action} {category} - {knowledge}\")\n",
    "    if knowledge_old:\n",
    "        print(f\"Old information: {knowledge_old}\")\n",
    "    \n",
    "    try:\n",
    "        with open(MEMORY_FILE, 'r') as f:\n",
    "            memories = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        memories = {cat.value: [] for cat in Category}\n",
    "    \n",
    "    # Ensure all categories exist in the memories\n",
    "    for cat in Category:\n",
    "        if cat.value not in memories:\n",
    "            memories[cat.value] = []\n",
    "    \n",
    "    if action == Action.ADD:\n",
    "        memories[category].append(knowledge)\n",
    "    elif action == Action.UPDATE:\n",
    "        if knowledge_old in memories[category]:\n",
    "            index = memories[category].index(knowledge_old)\n",
    "            memories[category][index] = knowledge\n",
    "        else:\n",
    "            # If the old knowledge is not found, add the new knowledge\n",
    "            memories[category].append(knowledge)\n",
    "    elif action == Action.DELETE:\n",
    "        memories[category] = [item for item in memories[category] if item != knowledge_old]\n",
    "    \n",
    "    with open(MEMORY_FILE, 'w') as f:\n",
    "        json.dump(memories, f)\n",
    "    \n",
    "    return memories\n",
    "\n",
    "tool_modify_knowledge = StructuredTool.from_function(\n",
    "    func=modify_knowledge,\n",
    "    name=\"Knowledge_Modifier\",\n",
    "    description=\"Add, update, or delete information in the dataset description\",\n",
    "    args_schema=AddKnowledge,\n",
    ")\n",
    "\n",
    "agent_tools = [tool_modify_knowledge]\n",
    "\n",
    "tool_executor = ToolExecutor(agent_tools)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt_memory_manager),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    streaming=True,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "tools = [convert_to_openai_function(t) for t in agent_tools]\n",
    "\n",
    "knowledge_master_runnable = prompt | llm.bind_tools(tools)\n",
    "\n",
    "# Set up the Graph\n",
    "class AgentState(TypedDict):\n",
    "    messages: Sequence[BaseMessage]\n",
    "    memories: Sequence[str]\n",
    "    contains_information: str\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.prebuilt import ToolInvocation\n",
    "\n",
    "def call_sentinel(state):\n",
    "    messages = state[\"messages\"]\n",
    "    response = sentinel_runnable.invoke(messages)\n",
    "    return {\"contains_information\": \"TRUE\" in response.content and \"yes\" or \"no\"}\n",
    "\n",
    "def should_continue(state):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if \"tool_calls\" not in last_message.additional_kwargs:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "def call_knowledge_master(state):\n",
    "    messages = state[\"messages\"]\n",
    "    memories = state[\"memories\"]\n",
    "    response = knowledge_master_runnable.invoke(\n",
    "        {\"messages\": messages, \"memories\": memories}\n",
    "    )\n",
    "    return {\"messages\": messages + [response]}\n",
    "\n",
    "def call_tool(state):\n",
    "    messages = state[\"messages\"]\n",
    "    memories = state[\"memories\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    for tool_call in last_message.additional_kwargs[\"tool_calls\"]:\n",
    "        action = ToolInvocation(\n",
    "            tool=tool_call[\"function\"][\"name\"],\n",
    "            tool_input=json.loads(tool_call[\"function\"][\"arguments\"]),\n",
    "            id=tool_call[\"id\"],\n",
    "        )\n",
    "\n",
    "        response = tool_executor.invoke(action)\n",
    "        function_message = ToolMessage(\n",
    "            content=str(response), name=action.tool, tool_call_id=tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "        messages.append(function_message)\n",
    "        if isinstance(response, dict) and \"updated_memories\" in response:\n",
    "            memories = response[\"updated_memories\"]\n",
    "\n",
    "    return {\"messages\": messages, \"memories\": memories}\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"sentinel\", call_sentinel)\n",
    "graph.add_node(\"knowledge_master\", call_knowledge_master)\n",
    "graph.add_node(\"action\", call_tool)\n",
    "\n",
    "graph.set_entry_point(\"sentinel\")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"sentinel\",\n",
    "    lambda x: x[\"contains_information\"],\n",
    "    {\n",
    "        \"yes\": \"knowledge_master\",\n",
    "        \"no\": END,\n",
    "    },\n",
    ")\n",
    "graph.add_conditional_edges(\n",
    "    \"knowledge_master\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"action\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "graph.add_edge(\"action\", END)\n",
    "\n",
    "app = graph.compile()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
