{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define a custom Reasoning and Action agent.\n",
    "\n",
    "Works with a chat model with tool calling support.\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, List, Literal, cast\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "from react_agent.configuration import Configuration\n",
    "from react_agent.state import InputState, State\n",
    "from react_agent.tools import TOOLS\n",
    "from react_agent.utils import load_chat_model\n",
    "\n",
    "# Define the function that calls the model\n",
    "\n",
    "\n",
    "async def call_model(\n",
    "    state: State, config: RunnableConfig\n",
    ") -> Dict[str, List[AIMessage]]:\n",
    "    \"\"\"Call the LLM powering our \"agent\".\n",
    "\n",
    "    This function prepares the prompt, initializes the model, and processes the response.\n",
    "\n",
    "    Args:\n",
    "        state (State): The current state of the conversation.\n",
    "        config (RunnableConfig): Configuration for the model run.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the model's response message.\n",
    "    \"\"\"\n",
    "    configuration = Configuration.from_runnable_config(config)\n",
    "\n",
    "    # Create a prompt template. Customize this to change the agent's behavior.\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\"system\", configuration.system_prompt), (\"placeholder\", \"{messages}\")]\n",
    "    )\n",
    "\n",
    "    # Initialize the model with tool binding. Change the model or add more tools here.\n",
    "    model = load_chat_model(configuration.model).bind_tools(TOOLS)\n",
    "\n",
    "    # Prepare the input for the model, including the current system time\n",
    "    message_value = await prompt.ainvoke(\n",
    "        {\n",
    "            \"messages\": state.messages,\n",
    "            \"system_time\": datetime.now(tz=timezone.utc).isoformat(),\n",
    "        },\n",
    "        config,\n",
    "    )\n",
    "\n",
    "    # Get the model's response\n",
    "    response = cast(AIMessage, await model.ainvoke(message_value, config))\n",
    "\n",
    "    # Handle the case when it's the last step and the model still wants to use a tool\n",
    "    if state.is_last_step and response.tool_calls:\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                AIMessage(\n",
    "                    id=response.id,\n",
    "                    content=\"Sorry, I could not find an answer to your question in the specified number of steps.\",\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    # Return the model's response as a list to be added to existing messages\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "\n",
    "workflow = StateGraph(State, input=InputState, config_schema=Configuration)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(call_model)\n",
    "workflow.add_node(\"tools\", ToolNode(TOOLS))\n",
    "\n",
    "# Set the entrypoint as `call_model`\n",
    "# This means that this node is the first one called\n",
    "workflow.add_edge(\"__start__\", \"call_model\")\n",
    "\n",
    "\n",
    "def route_model_output(state: State) -> Literal[\"__end__\", \"tools\"]:\n",
    "    \"\"\"Determine the next node based on the model's output.\n",
    "\n",
    "    This function checks if the model's last message contains tool calls.\n",
    "\n",
    "    Args:\n",
    "        state (State): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        str: The name of the next node to call (\"__end__\" or \"tools\").\n",
    "    \"\"\"\n",
    "    last_message = state.messages[-1]\n",
    "    if not isinstance(last_message, AIMessage):\n",
    "        raise ValueError(\n",
    "            f\"Expected AIMessage in output edges, but got {type(last_message).__name__}\"\n",
    "        )\n",
    "    # If there is no tool call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"__end__\"\n",
    "    # Otherwise we execute the requested actions\n",
    "    return \"tools\"\n",
    "\n",
    "\n",
    "# Add a conditional edge to determine the next step after `call_model`\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_model\",\n",
    "    # After call_model finishes running, the next node(s) are scheduled\n",
    "    # based on the output from route_model_output\n",
    "    route_model_output,\n",
    ")\n",
    "\n",
    "# Add a normal edge from `tools` to `call_model`\n",
    "# This creates a cycle: after using tools, we always return to the model\n",
    "workflow.add_edge(\"tools\", \"call_model\")\n",
    "\n",
    "# Compile the workflow into an executable graph\n",
    "# You can customize this by adding interrupt points for state updates\n",
    "graph = workflow.compile(\n",
    "    interrupt_before=[],  # Add node names here to update state before they're called\n",
    "    interrupt_after=[],  # Add node names here to update state after they're called\n",
    ")\n",
    "graph.name = \"ReAct Agent\"  # This customizes the name in LangSmith\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
